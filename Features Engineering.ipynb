{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering: Basic Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "'Feature engineering is the process of using domain knowledge to extract features from raw data. These features can be used to improve the performance of machine learning(ML) algorithms. Feature engineering can be considered as applied machine learning itself'.[Wikipedia](https://en.wikipedia.org/wiki/Feature_engineering)\n",
    "\n",
    "How features and what features are presented to an ML algorithm is very important, as this would greatly influence the qaulity of the ML algorithm result/output.  So much lies in the quality of the data and how data is presented.  \"Gabage In, Gabage Out\", \"Quality In, Quality Out\".\n",
    "\n",
    "At the heart of it, a machine learning problem is a knowledge representation problem.  How the knowledge is represented would determine if or how quickly the ML algorithm would provide results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many strokes can you count?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='uncounted_strokes.PNG',width=400, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many seconds did it take you to determine the number of strokes we have? Are there approaches we can apply to make this process rather simple or simpler? What if these strokes are grouped into groups of 5 and clustered together would it simplify the process for us?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='counted_strokes.PNG',width=400, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering  involves the the possible approaches that is required to make  it easier to present data to a machine learning algorithm to enable it  find insights and relationships between the features and the target variables\n",
    "\n",
    "The type of feature engineering approach we use is influenced by the type of data provided and the machine learning algorithm to be used.\n",
    "\n",
    "In this Jupyter notebook we would focus on numeric data type, categorical data type and time series.  Other data types taht would not be considered include text, images, speech etc. \n",
    "\n",
    "One approach to feature engineering that is often neglected and of great importance is engaging the SME. e.g height, weight and the interaction between the two features - body mass index (BMI). BMI is a better representation of being overweight than height or weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content\n",
    "\n",
    "1. Feature Engineering Approaches\n",
    "2. Feature Selection Approaches\n",
    "3. References "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "dataset = fetch_california_housing()\n",
    "\n",
    "retail_analytics_dataset = pd.read_csv(\"clean_retail_analytics_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.DataFrame(dataset['data'], columns=dataset['feature_names'])\n",
    "df_target = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features[dataset['feature_names'][:-2]].boxplot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize =(15,15))\n",
    "\n",
    "features_columns = df_features.columns.tolist()\n",
    "\n",
    "for x,y in enumerate(features_columns):\n",
    "    plt.subplot(4,3,x+1)\n",
    "    plt.hist(x=df_features[y], bins=30)\n",
    "    plt.title(y + ' distribution') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Feature Engineering Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Feature Transformation  \n",
    "b. Feature Extraction/Creation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Feature Transformation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scaling\n",
    "  - Normalization - tranforming range of values from between 0.0 to 1.0\n",
    "  - Standardization - transforming range of value such that the mean is 0 and the standard deviation is 1\n",
    "- Log Transformation - the log of a value. Other transformation like squared exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data range of features can have undue influence on a ML algorithm.  The impact varies per algorithm type. ML algorithms like linear regression, Support Vector machine  are influenced by this varying data range. Linear regression also expect some statistical assumption that lends itself to using standardization. Decision trees, random forest are less influenced by this.  In practice it is important the features are scaled.   \n",
    "\n",
    "It is important to note while Normalization reduces the range of outliers \"it does guarantee balanced feature scales in the presence of outliers\" [Scalers & Outliers](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html) \n",
    "\n",
    "Generally if you expect the feature to be normally distributed you would use standardization.  In practice it does not hurt to try both approaches.  Data Science is both a science and an art."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization - MinMaxScaler\n",
    "# Standardization - StandardScaler\n",
    "# Feature Transformation - PowerTransformer\n",
    "# Category variable transformation - Category\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PowerTransformer, OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df_features.loc[:,['MedInc', 'HouseAge', 'AveBedrms', 'Population']]\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.loc[:,['MedInc', 'HouseAge', 'AveBedrms', 'Population']].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize =(12,7))\n",
    "\n",
    "for x,y in enumerate(['MedInc', 'HouseAge', 'AveBedrms', 'Population']):\n",
    "    plt.subplot(2,3,x+1)\n",
    "    plt.hist(x=df_features[y], bins=30)\n",
    "    plt.title(y + ' distribution') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling using sklearn\n",
    "\n",
    "minmax = MinMaxScaler()\n",
    "df_normalized = minmax.fit_transform(df2)\n",
    "df_normalized = pd.DataFrame(df_normalized, columns=['MedInc', 'HouseAge', 'AveBedrms', 'Population'])\n",
    "df_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize =(12,7))\n",
    "\n",
    "for x,y in enumerate(df_normalized.columns.tolist()):\n",
    "    plt.subplot(2,3,x+1)\n",
    "    plt.hist(x=df_normalized[y], bins=30)\n",
    "    plt.title(y + ' distribution') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling using sklearn\n",
    "\n",
    "standard = StandardScaler()\n",
    "standard.fit(df2)\n",
    "df_standardized = standard.transform(df2)\n",
    "df_standardized = pd.DataFrame(df_standardized, columns=['MedInc', 'HouseAge', 'AveBedrms', 'Population'])\n",
    "df_standardized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_standardized.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize =(12,7))\n",
    "\n",
    "for x,y in enumerate(df_standardized.columns.tolist()):\n",
    "    plt.subplot(2,3,x+1)\n",
    "    plt.hist(x=df_standardized[y], bins=30)\n",
    "    plt.title(y + ' distribution') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looking at the histogram of the original dataset, the nomalization and standardization would do you observe in the scaling of the distribution of the features? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation\n",
    "- log transform\n",
    "- squared transform\n",
    "- Power transform (supports  Box-Cox and yeo-johnson transformation)\n",
    "\n",
    "Tranformation help to normalize originally  skewed data. Log transfrom is most frequently used.\n",
    "\n",
    "However when using log transform you must ensure that the values transformed are greater than zero as log(0) is undefined.Numpy caters for this by providing np.log1p "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### log transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log = df2[['MedInc', 'HouseAge', 'AveBedrms', 'Population']].applymap(np.log1p)\n",
    "df_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize =(12,7))\n",
    "\n",
    "for x,y in enumerate(['MedInc', 'HouseAge', 'AveBedrms', 'Population']):\n",
    "    plt.subplot(2,3,x+1)\n",
    "    plt.hist(x=df_log[y], bins=30)\n",
    "    plt.title(y + ' distribution') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Power Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power = PowerTransformer()\n",
    "power.fit(df2)\n",
    "df_power = power.transform(df2)\n",
    "df_power = pd.DataFrame(df_power, columns=['MedInc', 'HouseAge', 'AveBedrms', 'Population'])\n",
    "df_power.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_power.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize =(12,7))\n",
    "\n",
    "for x,y in enumerate(df_power.columns.tolist()):\n",
    "    plt.subplot(2,3,x+1)\n",
    "    plt.hist(x=df_power[y], bins=30)\n",
    "    plt.title(y + ' distribution') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Variable transformation\n",
    "1. OneHot encoding\n",
    "2. Creating categorical variables from numeric variables\n",
    "\n",
    "Categorical variables need to converted to numeric variables.  Two approaches are the label encoding and the OneHot encoding.  OneHot is most appropriate as Label encoding would introduce an order or some form of magnitude which may not exist in the dataset.  E.g. a categorical feature than contain Low, Medium, High when using Label encoding would be translated into 1,2,3 while when using OneHot encoding would be translated into [100],[010], [001]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_analytics_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_analytics_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting features to the appropriate variable type\n",
    "retail_analytics_dataset['region'] = retail_analytics_dataset['region'].astype('category')\n",
    "retail_analytics_dataset['order_date'] = pd.to_datetime(retail_analytics_dataset['order_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_analytics_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_analytics_dataset.region.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OneHot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehotencoder = OneHotEncoder()\n",
    "\n",
    "onehotencoder.fit(retail_analytics_dataset[['region']])\n",
    "encoded = onehotencoder.transform(retail_analytics_dataset[['region']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(retail_analytics_dataset['region']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas provides the *pd.get_dummies* method and scikit-learn offers the *OneHotEncoder* object.  What is the difference and when is it appropraite to use either?\n",
    "\n",
    "OneHotEncoding by scikit-learn enables us apply oneHotEncoding to new dataset using the same encoder created from previous  (original) data. pd.get_dummies does not afford that opportunity. You must transform categorical data ahead. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating categorical variables from numeric variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes there may be need to create a categorical variable from numeric variables.  E.g. grouping business based on their revenue as small, medium and large scale enterprise.  This can be achieved by creating bins from the numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features[['Population']].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.cut(df_features.Population, [0,1000,5000,35682])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features[['Population_group']] = pd.cut(df_features.Population, [0,1000,5000,35682], labels=['small', 'medium', 'large'])\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction & Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are instances where you would like to create additional features from existing features. This might be extracting features from a date field, or creating features based on the interaction of already known fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datetime features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_analytics_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create date features\n",
    "def quarter_checker(x):\n",
    "    if x in [1,2,3]:\n",
    "        # Qtr 1\n",
    "        return 1\n",
    "    elif x in [4,5,6]:\n",
    "          # Qtr 2\n",
    "        return 2\n",
    "    elif x in [7,8,9]:\n",
    "        # Qtr 3\n",
    "        return 3\n",
    "    else:\n",
    "        # Qtr 4\n",
    "        return 4\n",
    "    \n",
    "public_holidays_in_brazil = []\n",
    "\n",
    "def date_feature_engineering(df2, column_name, drop_others=False):\n",
    "    df =df2.copy()\n",
    "    if drop_others:\n",
    "        columns = list(df.columns)\n",
    "        columns.remove(column_name)\n",
    "        df.drop(columns, axis=True, inplace=True)\n",
    "    df[column_name + '_year'] = df[column_name].dt.year\n",
    "    df[column_name+ '_month'] = df[column_name].dt.month\n",
    "    df[column_name+ '_week'] = df[column_name].dt.isocalendar().week\n",
    "    df[column_name+ '_day'] = df[column_name].dt.day\n",
    "    df[column_name+ '_dayofweek'] = df[column_name].dt.dayofweek\n",
    "\n",
    "    df[column_name +'_ismonth_start'] =  df[column_name + '_day'] == 1\n",
    "    \n",
    "    df[column_name +'_isweekend'] =   df[column_name].dt.dayofweek.isin([5,6])\n",
    "    \n",
    "    df['order_quarter'] = df[column_name].dt.month.apply(lambda x: quarter_checker(x))\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_new_features = date_feature_engineering(retail_analytics_dataset,'order_date')\n",
    "retail_new_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building ML Models using the different Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target, dataset.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_updated = pd.get_dummies(df_features, columns=['Population_group'])\n",
    "df_updated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_updated.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_updated, df_target, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple regression model with no transformation\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "reg = LinearRegression()\n",
    "\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "print('model without scaling\\n')\n",
    "print('score:', reg.score(X_train, y_train))\n",
    "\n",
    "print ('\\ntraining rmse')\n",
    "print(mean_squared_error(y_train, reg.predict(X_train))** 0.5)\n",
    "\n",
    "\n",
    "print ('\\ntest rmse')\n",
    "print(mean_squared_error(y_test, reg.predict(X_test))** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple regression model with Normalization (MinMaxScaler)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# transform data\n",
    "minmax = MinMaxScaler()\n",
    "minmax.fit(X_train)\n",
    "\n",
    "X_train_transform = minmax.transform(X_train)\n",
    "X_test_transform = minmax.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "reg = LinearRegression()\n",
    "\n",
    "\n",
    "\n",
    "reg.fit(X_train_transform, y_train)\n",
    "\n",
    "print('model  with Normalization (MinMaxScaler)\\n')\n",
    "print('score:', reg.score(X_train_transform, y_train))\n",
    "\n",
    "print ('\\ntraining rmse')\n",
    "print(mean_squared_error(y_train, reg.predict(X_train_transform))** 0.5)\n",
    "\n",
    "\n",
    "print ('\\ntest rmse')\n",
    "print(mean_squared_error(y_test, reg.predict(X_test_transform))** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple regression model with Standardization (StandardScaler)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# transform data\n",
    "standard = StandardScaler()\n",
    "standard.fit(X_train)\n",
    "\n",
    "X_train_transform_s = standard.transform(X_train)\n",
    "X_test_transform_s= standard.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "reg = LinearRegression()\n",
    "\n",
    "\n",
    "\n",
    "reg.fit(X_train_transform_s, y_train)\n",
    "\n",
    "print('model  with Standardization (StandardScaler)\\n')\n",
    "print('score:', reg.score(X_train_transform_s, y_train))\n",
    "\n",
    "print ('\\ntraining rmse')\n",
    "print(mean_squared_error(y_train, reg.predict(X_train_transform_s))** 0.5)\n",
    "\n",
    "\n",
    "print ('\\ntest rmse')\n",
    "print(mean_squared_error(y_test, reg.predict(X_test_transform_s))** 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(2)\n",
    "poly.fit(X_train_transform)\n",
    "\n",
    "X_train_poly = poly.transform(X_train_transform)\n",
    "X_test_poly = poly.transform(X_test_transform)\n",
    "\n",
    "print('shape (no interaction): ', X_train_transform.shape)\n",
    "print('shape (interaction): ', X_train_poly.shape)\n",
    "\n",
    "\n",
    "reg = LinearRegression()\n",
    "\n",
    "\n",
    "reg.fit(X_train_poly, y_train)\n",
    "\n",
    "print('model  with interaction \\n')\n",
    "print('score:', reg.score(X_train_poly, y_train))\n",
    "\n",
    "print ('\\ntraining rmse')\n",
    "print(mean_squared_error(y_train, reg.predict(X_train_poly))** 0.5)\n",
    "\n",
    "\n",
    "print ('\\ntest rmse')\n",
    "print(mean_squared_error(y_test, reg.predict(X_test_poly))** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out features interaction\n",
    "print(poly.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "Several approaches exist for future selection. We would demonstrate Stepwise selection in this notebook. Other approaches include:\n",
    "- removing features with low variance\n",
    "- leveraging features importance property of RandomForest\n",
    "- Leveraging regularized linear regression model.\n",
    "- Dimension reduction techniques like PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential Feature Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(2)\n",
    "poly.fit(X_train_transform)\n",
    "\n",
    "X_train_poly = poly.transform(X_train_transform)\n",
    "X_test_poly = poly.transform(X_test_transform)\n",
    "\n",
    "print('shape (no interaction): ', X_train_transform.shape)\n",
    "print('shape (interaction): ', X_train_poly.shape)\n",
    "\n",
    "\n",
    "reg = LinearRegression()\n",
    "\n",
    "sfs = SequentialFeatureSelector(reg, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "sfs.fit(X_train_poly, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(2)\n",
    "poly.fit(X_train_transform)\n",
    "\n",
    "X_train_poly = poly.transform(X_train_transform)\n",
    "X_test_poly = poly.transform(X_test_transform)\n",
    "\n",
    "#reduce X to the selected features\n",
    "X_train_poly_reduced = sfs.transform(X_train_poly) \n",
    "X_test_poly_reduced = sfs.transform(X_test_poly) \n",
    "\n",
    "print('shape (no interaction): ', X_train_transform.shape)\n",
    "print('shape (interaction): ', X_train_poly.shape)\n",
    "print('shape (interaction with feature selection): ', X_train_poly_reduced.shape)\n",
    "\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train_poly_reduced, y_train)\n",
    "\n",
    "print('model  with interaction \\n')\n",
    "print('score:', reg.score(X_train_poly_reduced, y_train))\n",
    "\n",
    "print ('\\ntraining rmse')\n",
    "print(mean_squared_error(y_train, reg.predict(X_train_poly_reduced))** 0.5)\n",
    "\n",
    "\n",
    "print ('\\ntest rmse')\n",
    "print(mean_squared_error(y_test, reg.predict(X_test_poly_reduced))** 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest = RandomForestRegressor(random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "forest.fit(X_train_transform, y_train)\n",
    "\n",
    "print('model  - Random Foreset \\n')\n",
    "print('score:', forest.score(X_train_transform, y_train))\n",
    "\n",
    "print ('\\ntraining rmse')\n",
    "print(mean_squared_error(y_train, forest.predict(X_train_transform))** 0.5)\n",
    "\n",
    "\n",
    "print ('\\ntest rmse')\n",
    "print(mean_squared_error(y_test, forest.predict(X_test_transform))** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_importance = pd.DataFrame(forest.feature_importances_, X_train.columns).reset_index()\n",
    "features_importance.columns =['feature', 'measure']\n",
    "features_importance=features_importance.sort_values('measure', ascending=True)\n",
    "features_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh('feature','measure', data=features_importance)\n",
    "plt.title('Features Importance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a Regression Model using the Importance Features Identified using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple regression model with Normalization (MinMaxScaler)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# transform data\n",
    "minmax = MinMaxScaler()\n",
    "\n",
    "importance_features = ['MedInc', 'AveOccup','Latitude', 'Longitude', 'HouseAge', 'AveRooms', 'Population', 'AveBedrms']\n",
    "\n",
    "X_train2 = X_train.loc[:, importance_features]\n",
    "X_test2 = X_test.loc[:,importance_features]\n",
    "\n",
    "minmax.fit(X_train2)\n",
    "\n",
    "X_train_transform2 = minmax.transform(X_train2)\n",
    "X_test_transform2 = minmax.transform(X_test2)\n",
    "\n",
    "\n",
    "\n",
    "reg = LinearRegression()\n",
    "\n",
    "\n",
    "\n",
    "reg.fit(X_train_transform2, y_train)\n",
    "\n",
    "print('model  with Important Features Only\\n')\n",
    "print('score:', reg.score(X_train_transform2, y_train))\n",
    "\n",
    "print ('\\ntraining rmse')\n",
    "print(mean_squared_error(y_train, reg.predict(X_train_transform2))** 0.5)\n",
    "\n",
    "\n",
    "print ('\\ntest rmse')\n",
    "print(mean_squared_error(y_test, reg.predict(X_test_transform2))** 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference materials**\n",
    "1. [Kaggle Feature Engineering Course](https://www.kaggle.com/learn/feature-engineering)\n",
    "2. [Topic 6. Feature Engineering and Feature Selection](https://www.kaggle.com/kashnitsky/topic-6-feature-engineering-and-feature-selection)\n",
    "3. [Feature Selection using sklearn](https://scikit-learn.org/stable/modules/feature_selection.html)\n",
    "3. [Featuretools](https://www.featuretools.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
